{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f6c1cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T08:26:28.610844Z",
     "iopub.status.busy": "2023-10-15T08:26:28.609998Z",
     "iopub.status.idle": "2023-10-15T08:26:35.396087Z",
     "shell.execute_reply": "2023-10-15T08:26:35.394804Z"
    },
    "papermill": {
     "duration": 6.793353,
     "end_time": "2023-10-15T08:26:35.398825",
     "exception": false,
     "start_time": "2023-10-15T08:26:28.605472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing <処理中>...\n",
      "finalize_df:          id  target\n",
      "0         0       1\n",
      "1         2       1\n",
      "2         3       1\n",
      "3         9       0\n",
      "4        11       1\n",
      "...     ...     ...\n",
      "3258  10861       0\n",
      "3259  10865       0\n",
      "3260  10868       1\n",
      "3261  10874       0\n",
      "3262  10875       0\n",
      "\n",
      "[3263 rows x 2 columns]\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "Process complete <プログラム終了>\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import re\n",
    "print(\"Processing <処理中>...\")\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "\n",
    "## kwの欠損値対処\n",
    "train = train.fillna({\"keyword\":0,\n",
    "                     \"location\":0})\n",
    "test = test.fillna({\"keyword\":0,\n",
    "                   \"location\":0})\n",
    "                   \n",
    "# train_id = train[\"id\"]\n",
    "# train_kw = train[\"keyword\"]\n",
    "# train_loc = train[\"location\"]\n",
    "# train_txt = train[\"text\"]\n",
    "# train_tgt = train[\"target\"]\n",
    "\n",
    "# test_id = train[\"id\"]\n",
    "# test_kw = train[\"keyword\"]\n",
    "# test_loc = train[\"location\"]\n",
    "# test_txt = train[\"text\"]\n",
    "\n",
    "test_id = test[\"id\"]\n",
    "test_kw = test[\"keyword\"]\n",
    "# test_loc = test[\"location\"] # 使用しなかった\n",
    "# test_txt = test[\"text\"] # 使用しなかった\n",
    "                   \n",
    "\n",
    "# print(\"freq : \\n\", keyword_df.value_counts())\n",
    "keyword_dic = {}\n",
    "kw_total_dic = {}\n",
    "num = 1\n",
    "for word, id_, add_score in zip(train[\"keyword\"], train[\"id\"], train[\"target\"]):\n",
    "    ## 割合計算のための総数（分母）\n",
    "    if word != 0: # 欠損値Nanを0にしたので、それをカウントされないようにする\n",
    "        try:\n",
    "            kw_total_dic[word] += num\n",
    "        except KeyError:\n",
    "            kw_total_dic[word] = num\n",
    "\n",
    "        ## 割合計算のための出現回数（分子）\n",
    "        if add_score == 1:\n",
    "            try:\n",
    "                keyword_dic[word] += add_score\n",
    "            except KeyError:\n",
    "                keyword_dic[word] = add_score\n",
    "\n",
    "# kw_digital_dic = {key: 0 for key in kw_total_dic.keys()}\n",
    "kw_digital_dic = {}\n",
    "for word in train[\"keyword\"]:\n",
    "    if word != 0:\n",
    "        ## keyword_dic[word]には存在しないがkw_total_dic[num]に存在するもの同士の演算　という矛盾の例外処理\n",
    "        try:\n",
    "            kw_digital_dic[word] = round(keyword_dic[word] / kw_total_dic[word], 3) # 割合を求める\n",
    "    #         kw_debug_dic = kw_digital_dic.copy() # このブロック間のifとelseをコメントアウトすれば割合を見れられる。\n",
    "            ## 正解率30%以上のキーワードのみ保存 or 未満は0扱い\n",
    "            if round(keyword_dic[word] / kw_total_dic[word], 3) < 0.4:\n",
    "                kw_digital_dic[word] = -1\n",
    "    #           del kw_digital_dic[word] #消したら後で書き込むときに不便かも\n",
    "            else:\n",
    "                kw_digital_dic[word] = 1 # 任意の割合を越えた場合、バリューを1にする。\n",
    "\n",
    "        except KeyError:\n",
    "            kw_digital_dic[word] = -1\n",
    "\n",
    "\n",
    "# print(\"digital:\\n\", kw_digital_dic)\n",
    "# print(\"kw_debug_dic: \\n\", kw_debug_dic)\n",
    "\n",
    "\n",
    "# 内包表記は左側のforから処理される。ただし、一番左は最後なのに注意。\n",
    "# remove_lst = [\"the\", \"that\", \"this\", \"with\", \"like\", \"from\", \"have\", \"&amp;\", ] # その他、5h1hなど.正規表現で大文字を全部小文字にする🚩\n",
    "# word = [word for sentence in train[\"text\"] for word in sentence.split() if (len(word)) >= 4 and (word not in remove_lst)] # 4文字以上を取り出す。\n",
    "# attention_lst = [\"disaster\", \"catastrophe\"]\n",
    "# とりあえず形態素解析で名詞だけ抽出してみる。次は感情、動詞とか？\n",
    "\n",
    "\n",
    "## 🚩target=1のとき頻度が大きい単語を災害指定単語とする。locationなどと共通集合となったらデータ除去の下限を緩和する。\n",
    "loc_num_dic = {} # locは文字列型\n",
    "id_loc_dic = {} # locはリスト型\n",
    "# 区切り文字をリストに追加\n",
    "separators_lst = [\",\", \"-\", \"|\", \"||\", \"&\", \"/\", \"!\", \"?\"]\n",
    "num = 0\n",
    "for loc, id_, add_score in zip(train[\"location\"], test_id, train[\"target\"]):\n",
    "    if (loc != 0) and (add_score == 1) and (loc != re.search(\"\\d+\", loc)):\n",
    "        loc = loc.lower().replace(\".\", \"\")\n",
    "    \n",
    "        # 括弧で囲まれた文字列を削除\n",
    "        loc = re.sub(r'\\([^)]*\\)', '', loc)\n",
    "\n",
    "        ## 正規表現のパターンを作成\n",
    "        pattern = \"|\".join(map(re.escape, separators_lst))\n",
    "        # re.escape(文字列)はエスケープシーケンスを文字列として取り扱うための処理。separatorsはリストなので直接は使えない。\n",
    "        # map(関数, リストorイテラブル)で、関数をリスト等に適用させた新しいリストを生成。\n",
    "        \n",
    "        ## 正規表現で区切り文字が含まれているかチェック\n",
    "        if re.search(pattern, loc):\n",
    "            parts_lst = re.split(pattern, loc)\n",
    "            for loc_part in parts_lst:\n",
    "                \n",
    "                ## loc_partが全て数字の場合を除くための例外処理。\n",
    "                try:\n",
    "                    if int(loc_part):\n",
    "                        pass\n",
    "                    \n",
    "                except ValueError:\n",
    "                    ## 文頭と文末のスペース削除\n",
    "                    if loc_part != \"\":\n",
    "                        if loc_part[0] == \" \":\n",
    "                            loc_part = loc_part[1:]\n",
    "                    if len(loc_part) > 1:\n",
    "                        if loc_part[-1] == \" \":\n",
    "                            loc_part = loc_part[:-1]\n",
    "                            \n",
    "                    ## \"場所（リストではない）\"・\"場所の数\"のペアで保存する辞書にキーがすでにある場合とない場合での例外処理\n",
    "                    try:\n",
    "                        loc_num_dic[loc_part] += add_score\n",
    "                    except KeyError:\n",
    "                        loc_num_dic[loc_part] = add_score\n",
    "                        \n",
    "                    if loc_part != '': # なんかloc_partが''になることがあるので除去\n",
    "                        ## \"id\"・\"場所（リスト）\"のペアで保存する辞書にキーがすでにある場合とない場合での例外処理\n",
    "                        try:\n",
    "                                id_loc_dic[id_].append(loc_part)\n",
    "                        except KeyError:\n",
    "                            id_loc_dic[id_] = [] # バリュー（個々のloc）を格納するためのリスト\n",
    "                            id_loc_dic[id_].append(loc_part)\n",
    "        else:\n",
    "            try:    \n",
    "                loc_num_dic[loc] += add_score\n",
    "            except KeyError:\n",
    "                loc_num_dic[loc] = add_score                            \n",
    "            try:\n",
    "                id_loc_dic[id_].append(loc)\n",
    "            except KeyError:\n",
    "                id_loc_dic[id_] = []\n",
    "                id_loc_dic[id_].append(loc)\n",
    "\n",
    "# print(\"loc_num_dic:\\n\", loc_num_dic)\n",
    "# print(\"id_loc_dic: \", id_loc_dic)\n",
    "\n",
    "loc_num_dic = {loc: 1 if num >= 2 else -1 for loc, num in loc_num_dic.items()} # 指定回数以上出現したら,numを1に更新。そうじゃなかったら-1に更新。今は2回以上出現している場所(location)に限っている。\n",
    "        \n",
    "id_loc_digital_dic = dict.fromkeys(test_id, 0) # キーだけ（idだけ）キーを流用した辞書作成。初期バリューは第二引数0に設定。    \n",
    "## idと0,1値を対応させる\n",
    "for id_, loc_lst in id_loc_dic.items():\n",
    "    for loc in loc_lst:\n",
    "        if loc_num_dic[loc] == 1:\n",
    "            ## idとそのnumをペアに持つ辞書に1を保存。id_loc_digital_dicにはデフォルトバリューを0に設定したので、含まれていたら1に更新するだけで良い。\n",
    "            id_loc_digital_dic[id_] = 1 \n",
    "        elif loc_num_dic[loc] == -1:\n",
    "            id_loc_digital_dic[id_] = -1 # 頻度が小さくて切り捨てられた場所(loc)を格納。\n",
    "\n",
    "# print(\"id_loc_dic(digital):\", id_loc_dic)\n",
    "# print(\"id_num_dic:\", id_num_dic)\n",
    "# print(\"id_len: \", len(test_id))\n",
    "# print(\"dic_len: \", len(id_loc_dic))\n",
    "# print(\"id_loc_digital_dic:\", id_loc_digital_dic)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_noun_dic = {}\n",
    "id_word_dic = {id_: [] for id_ in test_id} # あらかじめ空リストを初期値として作っておいた\n",
    "# print(\"###\", id_word_dic)\n",
    "no_need_lst = [\"http\", \"https\", \"amp\", \"@\"] # 🚩\n",
    "for id_, sentence, add_score in zip(test_id, train[\"text\"], train[\"target\"]):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_lst = nltk.pos_tag(tokens)\n",
    "    \n",
    "    for word, pos in tagged_lst:\n",
    "        # 名詞,つまりNNのみ取得\n",
    "        if (pos == \"NN\") and (word not in no_need_lst) and (add_score != 0):\n",
    "            try:\n",
    "                text_noun_dic[word] += add_score # row_idxとwordを紐づけたい...辞書型にするとか？\n",
    "            except KeyError:\n",
    "                text_noun_dic[word] = add_score\n",
    "                \n",
    "            ## idと単語の組み合わせを保存\n",
    "            id_word_dic[id_].append(word)\n",
    "\n",
    "text_noun_dic = {word: 1 if num >= 2 else -1 for word, num in text_noun_dic.items()} # 頻度の小さい単語の切り捨て.今は取り合えず2以上にしてる\n",
    "\n",
    "id_word_digital_dic = dict.fromkeys(test_id, 0)\n",
    "## idと0,1とを対応させる\n",
    "for id_, word_lst in id_word_dic.items():\n",
    "# for id_, word_lst in zip(test_id, id_word_dic.values()):\n",
    "    for word in word_lst:\n",
    "        if text_noun_dic[word] == 1:\n",
    "            id_word_digital_dic[id_] = 1\n",
    "        elif text_noun_dic[word] == -1:\n",
    "            id_word_digital_dic[id_] = -1\n",
    "\n",
    "\n",
    "## 前処理後のリスト\n",
    "kw_digital_lst = [kw_digital_dic.get(kw, 0) for kw in test_kw] # kwが存在すればその値を返し、存在しなければ欠損値として0を返している。-1は切り捨てられたキーワード。\n",
    "loc_digital_lst = list(id_loc_digital_dic.values())\n",
    "word_digital_lst = list(id_word_digital_dic.values())\n",
    "# print(\"id: \", len(test_id))\n",
    "# print(\"kw: \", len(kw_digital_lst))\n",
    "# print(\"loc: \", len(loc_digital_lst))\n",
    "# print(\"text: \", len(word_digital_lst))\n",
    "\n",
    "\n",
    "## 確認debug用データフレーム\n",
    "# preprocessing = pd.DataFrame(\n",
    "#                     data = {\"<id>\": test_id,\n",
    "#                             \"<keyword>\": kw_digital_lst,\n",
    "#                            \"<location>\": loc_digital_lst,\n",
    "#                            \"<text>\": word_digital_lst,\n",
    "#                             \"true_target\":train_tgt\n",
    "#                            }\n",
    "# )\n",
    "# print(\"🚩preprocessing_dataFrame:\\n\", preprocessing[30:50])\n",
    "\n",
    "prepared_lst = []\n",
    "\n",
    "for kw, loc, word in zip(kw_digital_lst, loc_digital_lst, word_digital_lst):\n",
    "    ## kwの判定\n",
    "    if kw != -1 and word != -1 and (kw != 0 or word != 0): # kwとwordが切り捨て値ではないかつ欠損値でもない場合\n",
    "        prepared_lst.append(1)\n",
    "    ## locの判定\n",
    "    elif loc: # loc=1のとき\n",
    "        prepared_lst.append(1)\n",
    "    ## wordの判定\n",
    "    elif kw == 0 and loc == 0 and word == 1: # 両方とも欠損値のとき、wordが1ならば\n",
    "        prepared_lst.append(1)\n",
    "    else:\n",
    "        prepared_lst.append(0)\n",
    "\n",
    "finalize_df = pd.DataFrame({\n",
    "                    \"id\":test_id,\n",
    "                    \"target\":prepared_lst\n",
    "})\n",
    "print(\"finalize_df:\", finalize_df)\n",
    "\n",
    "finalize_df.to_csv(\"disaster_tweet.csv\", index = False)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "## 32番目からキーワードと場所\n",
    "## targetは0から14行(id=20)以降0\n",
    "## 🚩text反省　精度が悪い。名詞のみでやってみたが、flood,floodedやfloodingは何判定？動詞？ 形態素解析に追加した方が良いかも。\n",
    "### 重み付け基準：　location > keyword > text\n",
    "### locationがあったら問答無用でtrue, keywordはtextも1ならtrue, textはそれ単体のときのみ(kw,locが0のとき)採用\n",
    "print(\"Process complete <プログラム終了>\")\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c51888",
   "metadata": {
    "papermill": {
     "duration": 0.001768,
     "end_time": "2023-10-15T08:26:35.404872",
     "exception": false,
     "start_time": "2023-10-15T08:26:35.403104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.745397,
   "end_time": "2023-10-15T08:26:36.128342",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-15T08:26:25.382945",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
